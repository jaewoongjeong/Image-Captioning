{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302cf690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call tf.config.experimental.set_memory_growth(GPU0, True)\n"
     ]
    }
   ],
   "source": [
    "from module import Preprocess, Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9798cb",
   "metadata": {},
   "source": [
    "# Global Variable & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6078d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_console(message):\n",
    "    print('-' * 10 + message + '-' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8b366",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428be217",
   "metadata": {},
   "source": [
    "##### Loading Preprocess Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "027f9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(DATASET='ms_coco', print_console=print_console)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3613eeb9",
   "metadata": {},
   "source": [
    "##### Loading Dataset\n",
    "- It loads ```DATASET_SIZE``` size of pictures and captions.\n",
    "- It returns:\n",
    "    - ```train_ids```: Contains IDs of all the images\n",
    "    - ```train_captions```: Contains captions of all the images\n",
    "        - It contains ***start*** and ***end*** captions\n",
    "    - ```img_name_vector```: Contains directory of all the images\n",
    "    - ```corpus```: Contains reference sentences of all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06bbe61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Loading MS-COCO Dataset----------\n",
      "----------Finished Loading Dataset----------\n"
     ]
    }
   ],
   "source": [
    "train_ids, train_captions, img_name_vector, corpus = preprocess.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c19723",
   "metadata": {},
   "source": [
    "##### Extracting Features using Inceptionv3 and YOLOv4\n",
    "- It extracts using Inceptionv3 ann YOLOv4 models and concatenates the results, and saving the file in .npy extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.feature_extraction(img_name_vector=img_name_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c364ec",
   "metadata": {},
   "source": [
    "##### Tokenizing\n",
    "- It tokenizes the ```train_captions``` and also pads each vector to the max length of the captions\n",
    "- It returns:\n",
    "    - ```tokenizer```: Keras preprocessing module that has fit ```train_captions```\n",
    "    - ```cap_vector```: It pads each vector to the max length of the caption\n",
    "    - ```max_length```: It contains the max length of caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e444b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Tokenizing Captions----------\n",
      "----------Finished Tokenizing Captions----------\n"
     ]
    }
   ],
   "source": [
    "tokenizer, cap_vector, max_length = preprocess.tokenize(train_captions=train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e8b6fe",
   "metadata": {},
   "source": [
    "##### Splitting Dataset into Train and Validation Set\n",
    "- It splits the ```train_ids```, ```img_name_vector``` and ```cap_vector``` into train, validation set with proportion of ```TEST_SET_PROPORTION```\n",
    "- It returns:\n",
    "    - ```img_name_train```: Training set that contains full directory of images\n",
    "    - ```cap_train```: Training set that contains tokenized padded captions of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65274d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Splitting Dataset into Train and Validation Sets----------\n",
      "----------Splitting Dataset into Train and Validation Sets----------\n"
     ]
    }
   ],
   "source": [
    "_, _, img_name_train, _, cap_train, _ = preprocess.train_test_split(train_ids=train_ids,\n",
    "                                                                    img_name_vector=img_name_vector,\n",
    "                                                                    cap_vector=cap_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14abd0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b8aa6",
   "metadata": {},
   "source": [
    "##### Loading Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ae17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Train(print_console=print_console, NUM_STEPS=len(img_name_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477f2342",
   "metadata": {},
   "source": [
    "##### Creating a Dataset for training\n",
    "- It creates a dataset using TensorFlow module for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97913f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Making Dataset for Training----------\n",
      "----------Finished Making Dataset for Training----------\n"
     ]
    }
   ],
   "source": [
    "dataset = trainer.make_dataset(img_name_train=img_name_train, cap_train=cap_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2528fea",
   "metadata": {},
   "source": [
    "##### Training\n",
    "- It trains for ```EPOCHS``` number of epochs if ```RESET``` is true, else loads previously saved weights\n",
    "    - It saves the weights as ```model_weights``` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2add1ce",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Start Training----------\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "1-th value returned by pyfunc_0 is int32, but expects int64\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JAEWOO~1\\AppData\\Local\\Temp/ipykernel_11676/2938764772.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\Programming\\Python\\WorkSpace\\Research Project\\Image Captioning\\Image_Captioning\\module.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, tokenizer, dataset)\u001b[0m\n\u001b[0;32m    358\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m                     batch_loss, t_loss = self.train_step(img_tensor=img_tensor, encoder=encoder, decoder=decoder,\n\u001b[0;32m    362\u001b[0m                                                          optimizer=optimizer, tokenizer=tokenizer, target=target)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    759\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[0;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2725\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2726\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2727\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2728\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2729\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6940\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6941\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6942\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: 1-th value returned by pyfunc_0 is int32, but expects int64\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "trainer.train(tokenizer=tokenizer, dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe9b85",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f49e49",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
